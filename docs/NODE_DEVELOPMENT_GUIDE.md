# èŠ‚ç‚¹å¼€å‘æŒ‡å—

æœ¬æŒ‡å—å°†å¸®åŠ©æ‚¨å¼€å‘è‡ªå®šä¹‰èŠ‚ç‚¹ï¼Œæ”¯æŒæµå¼å’Œéæµå¼æ•°æ®å¤„ç†ã€‚

## ç›®å½•

1. [èŠ‚ç‚¹åŸºç¡€](#èŠ‚ç‚¹åŸºç¡€)
2. [èŠ‚ç‚¹æ³¨å†Œ](#èŠ‚ç‚¹æ³¨å†Œ)
3. [å‚æ•°å®šä¹‰è§„èŒƒ](#å‚æ•°å®šä¹‰è§„èŒƒ)
4. [æµå¼èŠ‚ç‚¹å¼€å‘](#æµå¼èŠ‚ç‚¹å¼€å‘)
5. [éæµå¼èŠ‚ç‚¹å¼€å‘](#éæµå¼èŠ‚ç‚¹å¼€å‘)
6. [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
7. [å®Œæ•´ç¤ºä¾‹](#å®Œæ•´ç¤ºä¾‹)

---

## èŠ‚ç‚¹åŸºç¡€

### ç»§æ‰¿ Node åŸºç±»

æ‰€æœ‰è‡ªå®šä¹‰èŠ‚ç‚¹éƒ½å¿…é¡»ç»§æ‰¿ `Node` åŸºç±»ï¼š

```python
from workflow_engine.core import Node, ParameterSchema, WorkflowContext

class MyCustomNode(Node):
    """æˆ‘çš„è‡ªå®šä¹‰èŠ‚ç‚¹"""
    
    # å®šä¹‰è¾“å…¥å‚æ•°ï¼ˆç±»å±æ€§ï¼‰
    INPUT_PARAMS = {}
    
    # å®šä¹‰è¾“å‡ºå‚æ•°ï¼ˆç±»å±æ€§ï¼‰
    OUTPUT_PARAMS = {}
    
    async def execute_async(self, context: WorkflowContext):
        """æ‰§è¡ŒèŠ‚ç‚¹é€»è¾‘"""
        pass
```

### å¿…éœ€å®ç°çš„æ–¹æ³•

- `execute_async(self, context)`: å¼‚æ­¥æ‰§è¡ŒèŠ‚ç‚¹é€»è¾‘ï¼ˆæ–°æ¶æ„ï¼‰
- æˆ– `execute(self, context)`: åŒæ­¥æ‰§è¡ŒèŠ‚ç‚¹é€»è¾‘ï¼ˆæ—§æ¶æ„å…¼å®¹ï¼‰

---

## èŠ‚ç‚¹æ³¨å†Œ

èŠ‚ç‚¹å¼€å‘å®Œæˆåï¼Œéœ€è¦æ³¨å†Œåˆ°å·¥ä½œæµå¼•æ“æ‰èƒ½ä½¿ç”¨ã€‚æ”¯æŒä¸‰ç§æ³¨å†Œæ–¹å¼ï¼š

### æ–¹å¼ 1ï¼šæ‰‹åŠ¨æ³¨å†Œï¼ˆçµæ´»ï¼‰

```python
from workflow_engine import WorkflowEngine
from my_nodes import MyCustomNode

engine = WorkflowEngine()
engine.register_node_type('my_custom', MyCustomNode)
```

**ä¼˜ç‚¹**ï¼šå®Œå…¨æ§åˆ¶æ³¨å†Œè¿‡ç¨‹ï¼Œé€‚åˆåŠ¨æ€åŠ è½½èŠ‚ç‚¹  
**ç¼ºç‚¹**ï¼šéœ€è¦æ‰‹åŠ¨ç®¡ç†æ³¨å†Œä»£ç 

### æ–¹å¼ 2ï¼šè‡ªåŠ¨æ³¨å†Œï¼ˆä¾¿æ·ï¼‰

å°†èŠ‚ç‚¹æ·»åŠ åˆ° `workflow_engine/nodes/__init__.py` çš„ `BUILTIN_NODES` å­—å…¸ï¼š

```python
# workflow_engine/nodes/__init__.py
from .my_custom_node import MyCustomNode

BUILTIN_NODES = {
    # ... å…¶ä»–èŠ‚ç‚¹
    'my_custom': MyCustomNode,
}
```

ç„¶åä½¿ç”¨è‡ªåŠ¨æ³¨å†Œå‡½æ•°ï¼š

```python
from workflow_engine import WorkflowEngine
from workflow_engine.nodes import auto_register_nodes

engine = WorkflowEngine()
auto_register_nodes(engine)  # è‡ªåŠ¨æ³¨å†Œæ‰€æœ‰å†…ç½®èŠ‚ç‚¹
```

**ä¼˜ç‚¹**ï¼šä¸€æ¬¡æ³¨å†Œæ‰€æœ‰èŠ‚ç‚¹ï¼Œä»£ç ç®€æ´  
**ç¼ºç‚¹**ï¼šéœ€è¦ä¿®æ”¹å†…ç½®æ¨¡å—æ–‡ä»¶

### æ–¹å¼ 3ï¼šè£…é¥°å™¨æ³¨å†Œï¼ˆä¼˜é›…ï¼‰

ä½¿ç”¨ `@register_node` è£…é¥°å™¨ï¼š

```python
from workflow_engine.core import Node, register_node

@register_node('my_custom')
class MyCustomNode(Node):
    """æˆ‘çš„è‡ªå®šä¹‰èŠ‚ç‚¹"""
    pass
```

ç„¶åè‡ªåŠ¨åŠ è½½æ‰€æœ‰è£…é¥°å™¨æ ‡è®°çš„èŠ‚ç‚¹ï¼š

```python
from workflow_engine import WorkflowEngine
from workflow_engine.core.node import get_registered_nodes

engine = WorkflowEngine()

# å¯¼å…¥åŒ…å«è£…é¥°å™¨èŠ‚ç‚¹çš„æ¨¡å—
import my_nodes  # ç¡®ä¿è£…é¥°å™¨ä»£ç è¢«æ‰§è¡Œ

# è‡ªåŠ¨æ³¨å†Œæ‰€æœ‰è£…é¥°å™¨æ ‡è®°çš„èŠ‚ç‚¹
for node_type, node_class in get_registered_nodes().items():
    engine.register_node_type(node_type, node_class)
```

**ä¼˜ç‚¹**ï¼šå£°æ˜å¼ï¼Œä»£ç æ¸…æ™°ï¼Œä¸éœ€è¦ä¿®æ”¹å…¶ä»–æ–‡ä»¶  
**ç¼ºç‚¹**ï¼šéœ€è¦ç¡®ä¿æ¨¡å—è¢«å¯¼å…¥

### èŠ‚ç‚¹ç±»å‹å‘½åè§„èŒƒ

- ä½¿ç”¨å°å†™å­—æ¯å’Œä¸‹åˆ’çº¿ï¼š`my_custom_node`
- æè¿°èŠ‚ç‚¹åŠŸèƒ½ï¼š`audio_processor`, `text_classifier`
- é¿å…ä¸å†…ç½®èŠ‚ç‚¹å†²çª

---

## å‚æ•°å®šä¹‰è§„èŒƒ

### å‚æ•°ç»“æ„å®šä¹‰

èŠ‚ç‚¹çš„è¾“å…¥è¾“å‡ºå‚æ•°é€šè¿‡ç±»å±æ€§ `INPUT_PARAMS` å’Œ `OUTPUT_PARAMS` å®šä¹‰ï¼š

```python
class MyNode(Node):
    INPUT_PARAMS = {
        "param_name": ParameterSchema(
            is_streaming=False,  # æ˜¯å¦æµå¼å‚æ•°
            schema="string"      # å‚æ•°ç»“æ„å®šä¹‰
        )
    }
    
    OUTPUT_PARAMS = {
        "result": ParameterSchema(
            is_streaming=False,
            schema="string"
        )
    }
```

### æ”¯æŒçš„æ•°æ®ç±»å‹

#### ç®€å•ç±»å‹

```python
schema="string"    # å­—ç¬¦ä¸²
schema="integer"   # æ•´æ•°
schema="float"     # æµ®ç‚¹æ•°
schema="boolean"   # å¸ƒå°”å€¼
schema="bytes"     # å­—èŠ‚æ•°æ®
schema="dict"      # å­—å…¸
schema="list"      # åˆ—è¡¨
schema="any"       # ä»»æ„ç±»å‹
```

#### ç»“æ„ä½“ï¼ˆå­—å…¸ç±»å‹ï¼‰

```python
schema={
    "field1": "string",
    "field2": "integer",
    "field3": "float"
}
```

### æµå¼å‚æ•° vs éæµå¼å‚æ•°

#### éæµå¼å‚æ•°

```python
ParameterSchema(
    is_streaming=False,
    schema="string"
)
```

- ä¸€æ¬¡æ€§ä¼ é€’å®Œæ•´æ•°æ®
- é€‚ç”¨äºï¼šé…ç½®ã€æ‰¹é‡æ•°æ®ã€æœ€ç»ˆç»“æœ

#### æµå¼å‚æ•°

```python
ParameterSchema(
    is_streaming=True,
    schema={
        "data": "bytes",
        "timestamp": "float"
    }
)
```

- æ•°æ®ä»¥ chunk å½¢å¼å¢é‡ä¼ é€’
- é€‚ç”¨äºï¼šéŸ³é¢‘æµã€è§†é¢‘æµã€å®æ—¶æ–‡æœ¬

---

## æµå¼èŠ‚ç‚¹å¼€å‘

### åŸºæœ¬ç»“æ„

```python
from workflow_engine.core import Node, ParameterSchema, StreamChunk, WorkflowContext
import asyncio

class StreamingNode(Node):
    """æµå¼èŠ‚ç‚¹ç¤ºä¾‹"""
    
    INPUT_PARAMS = {
        "stream_in": ParameterSchema(
            is_streaming=True,
            schema={"data": "bytes"}
        )
    }
    
    OUTPUT_PARAMS = {
        "stream_out": ParameterSchema(
            is_streaming=True,
            schema={"data": "bytes", "processed": "boolean"}
        )
    }
    
    async def execute_async(self, context: WorkflowContext):
        """åˆå§‹åŒ–ï¼ˆæŒç»­è¿è¡Œï¼‰"""
        context.log(f"æµå¼èŠ‚ç‚¹å¯åŠ¨: {self.node_id}")
        try:
            await asyncio.sleep(float('inf'))  # ä¿æŒè¿è¡Œ
        except asyncio.CancelledError:
            context.log(f"æµå¼èŠ‚ç‚¹å…³é—­: {self.node_id}")
    
    async def on_chunk_received(self, param_name: str, chunk: StreamChunk):
        """å¤„ç†æ¥æ”¶åˆ°çš„ chunk"""
        if param_name == "stream_in":
            # å¤„ç†è¾“å…¥æ•°æ®
            input_data = chunk.data["data"]
            processed_data = self._process(input_data)
            
            # å‘é€è¾“å‡º chunk
            await self.emit_chunk("stream_out", {
                "data": processed_data,
                "processed": True
            })
    
    def _process(self, data):
        """æ•°æ®å¤„ç†é€»è¾‘"""
        return data  # ç¤ºä¾‹
```

### å…³é”®æ–¹æ³•

#### `async def execute_async(self, context)`

- èŠ‚ç‚¹åˆå§‹åŒ–é€»è¾‘
- é€šå¸¸æŒç»­è¿è¡Œï¼ˆ`await asyncio.sleep(float('inf'))`ï¼‰
- å¯ä»¥åœ¨è¿™é‡Œåˆå§‹åŒ–æ¨¡å‹ã€å»ºç«‹è¿æ¥ç­‰

#### `async def on_chunk_received(self, param_name, chunk)`

- å¤„ç†æ¥æ”¶åˆ°çš„æµå¼æ•°æ®
- `param_name`: è¾“å…¥å‚æ•°åç§°
- `chunk.data`: æ•°æ®å­—å…¸ï¼ˆæ ¹æ® schema éªŒè¯ï¼‰

#### `async def emit_chunk(self, param_name, chunk_data)`

- å‘é€æµå¼è¾“å‡ºæ•°æ®
- `param_name`: è¾“å‡ºå‚æ•°åç§°
- `chunk_data`: æ•°æ®å­—å…¸ï¼ˆä¼šè‡ªåŠ¨éªŒè¯ï¼‰

### æµå¼èŠ‚ç‚¹ç”Ÿå‘½å‘¨æœŸ

```
1. å¼•æ“å¯åŠ¨ -> è°ƒç”¨ __init__
2. å¯åŠ¨ consume_stream åå°ä»»åŠ¡ï¼ˆç›‘å¬è¾“å…¥ï¼‰
3. è°ƒç”¨ execute_asyncï¼ˆåˆå§‹åŒ–ï¼‰
4. æŒç»­è¿è¡Œï¼Œå¤„ç†è¾“å…¥ chunk
5. æ¥æ”¶ç»“æŸä¿¡å· -> é€€å‡º consume_stream
6. asyncio.CancelledError -> execute_async é€€å‡º
```

---

## éæµå¼èŠ‚ç‚¹å¼€å‘

### åŸºæœ¬ç»“æ„

```python
from workflow_engine.core import Node, ParameterSchema, WorkflowContext

class BatchNode(Node):
    """æ‰¹é‡å¤„ç†èŠ‚ç‚¹ç¤ºä¾‹"""
    
    INPUT_PARAMS = {
        "input_data": ParameterSchema(
            is_streaming=False,
            schema="dict"
        )
    }
    
    OUTPUT_PARAMS = {
        "output_data": ParameterSchema(
            is_streaming=False,
            schema="dict"
        )
    }
    
    async def execute_async(self, context: WorkflowContext):
        """æ‰§è¡Œæ‰¹é‡å¤„ç†"""
        # è·å–è¾“å…¥æ•°æ®
        input_data = self.get_input_value("input_data")
        
        # å¤„ç†æ•°æ®
        result = self._process(input_data)
        
        # è®¾ç½®è¾“å‡º
        self.set_output_value("output_data", result)
        
        context.log(f"æ‰¹é‡å¤„ç†å®Œæˆ: {self.node_id}")
    
    def _process(self, data):
        """æ•°æ®å¤„ç†é€»è¾‘"""
        return {"processed": True, "data": data}
```

### å…³é”®æ–¹æ³•

#### `self.get_input_value(param_name)`

- è·å–éæµå¼è¾“å…¥å‚æ•°çš„å€¼
- è¿”å›å€¼å·²é€šè¿‡ schema éªŒè¯

#### `self.set_output_value(param_name, value)`

- è®¾ç½®éæµå¼è¾“å‡ºå‚æ•°çš„å€¼
- å€¼ä¼šè‡ªåŠ¨éªŒè¯å¹¶ä¿å­˜

---

## æœ€ä½³å®è·µ

### 1. é”™è¯¯å¤„ç†

```python
async def execute_async(self, context: WorkflowContext):
    try:
        # æ‰§è¡Œé€»è¾‘
        result = await self._do_something()
    except ValueError as e:
        context.log(f"å‚æ•°é”™è¯¯: {e}", level="ERROR")
        raise
    except Exception as e:
        context.log(f"æœªçŸ¥é”™è¯¯: {e}", level="ERROR")
        raise
```

### 2. æ—¥å¿—è®°å½•

```python
context.log("å¼€å§‹å¤„ç†", level="INFO")
context.log("å¤„ç†æˆåŠŸ", level="SUCCESS")
context.log("è­¦å‘Šä¿¡æ¯", level="WARNING")
context.log("é”™è¯¯ä¿¡æ¯", level="ERROR")
```

### 3. é…ç½®å‚æ•°

```python
class MyNode(Node):
    async def execute_async(self, context: WorkflowContext):
        # ä»é…ç½®ä¸­è·å–å‚æ•°ï¼Œæä¾›é»˜è®¤å€¼
        threshold = self.config.get('threshold', 0.5)
        max_retries = self.config.get('max_retries', 3)
        
        context.log(f"é…ç½®: threshold={threshold}, max_retries={max_retries}")
```

### 4. å‚æ•°éªŒè¯

```python
async def execute_async(self, context: WorkflowContext):
    # éªŒè¯å¿…éœ€é…ç½®
    if 'api_key' not in self.config:
        raise ValueError("ç¼ºå°‘å¿…éœ€é…ç½®: api_key")
    
    # éªŒè¯å‚æ•°èŒƒå›´
    temperature = self.config.get('temperature', 0.7)
    if not 0 <= temperature <= 2:
        raise ValueError(f"temperature å¿…é¡»åœ¨ 0-2 ä¹‹é—´ï¼Œå½“å‰: {temperature}")
```

### 5. èµ„æºç®¡ç†

```python
class MyNode(Node):
    def __init__(self, node_id, config, connection_manager=None):
        super().__init__(node_id, config, connection_manager)
        self.resource = None
    
    async def execute_async(self, context: WorkflowContext):
        try:
            # åˆå§‹åŒ–èµ„æº
            self.resource = await self._init_resource()
            
            # æ‰§è¡Œé€»è¾‘
            await asyncio.sleep(float('inf'))
        finally:
            # æ¸…ç†èµ„æº
            if self.resource:
                await self.resource.close()
```

### 6. æ€§èƒ½ä¼˜åŒ–

```python
async def on_chunk_received(self, param_name: str, chunk: StreamChunk):
    # é¿å…åœ¨æ¯ä¸ª chunk ä¸­é‡å¤åˆå§‹åŒ–
    if not hasattr(self, '_processor'):
        self._processor = self._create_processor()
    
    # æ‰¹é‡å¤„ç†ï¼ˆå‡å°‘ I/Oï¼‰
    self._buffer.append(chunk.data)
    if len(self._buffer) >= self.batch_size:
        await self._process_batch(self._buffer)
        self._buffer.clear()
```

---

## å®Œæ•´ç¤ºä¾‹

### ç¤ºä¾‹ 1ï¼šæµå¼éŸ³é¢‘å¤„ç†èŠ‚ç‚¹

```python
from workflow_engine.core import Node, ParameterSchema, StreamChunk, WorkflowContext, register_node
import asyncio

@register_node('audio_processor')
class AudioProcessorNode(Node):
    """éŸ³é¢‘å¤„ç†èŠ‚ç‚¹"""
    
    INPUT_PARAMS = {
        "audio_in": ParameterSchema(
            is_streaming=True,
            schema={
                "audio_data": "bytes",
                "sample_rate": "integer",
                "format": "string"
            }
        )
    }
    
    OUTPUT_PARAMS = {
        "audio_out": ParameterSchema(
            is_streaming=True,
            schema={
                "audio_data": "bytes",
                "sample_rate": "integer",
                "format": "string",
                "processed": "boolean"
            }
        )
    }
    
    async def execute_async(self, context: WorkflowContext):
        """åˆå§‹åŒ–éŸ³é¢‘å¤„ç†å™¨"""
        self.effect = self.config.get('effect', 'none')
        context.log(f"éŸ³é¢‘å¤„ç†å™¨å¯åŠ¨ï¼Œæ•ˆæœ: {self.effect}")
        
        try:
            await asyncio.sleep(float('inf'))
        except asyncio.CancelledError:
            context.log("éŸ³é¢‘å¤„ç†å™¨å…³é—­")
    
    async def on_chunk_received(self, param_name: str, chunk: StreamChunk):
        """å¤„ç†éŸ³é¢‘ chunk"""
        if param_name == "audio_in":
            audio_data = chunk.data["audio_data"]
            sample_rate = chunk.data["sample_rate"]
            
            # åº”ç”¨éŸ³é¢‘æ•ˆæœ
            processed_audio = self._apply_effect(audio_data)
            
            # å‘é€å¤„ç†åçš„éŸ³é¢‘
            await self.emit_chunk("audio_out", {
                "audio_data": processed_audio,
                "sample_rate": sample_rate,
                "format": chunk.data["format"],
                "processed": True
            })
    
    def _apply_effect(self, audio_data: bytes) -> bytes:
        """åº”ç”¨éŸ³é¢‘æ•ˆæœ"""
        # å®é™…å®ç°ï¼šåº”ç”¨é™å™ªã€å‡è¡¡å™¨ç­‰æ•ˆæœ
        return audio_data
```

### ç¤ºä¾‹ 2ï¼šæ‰¹é‡æ•°æ®å¤„ç†èŠ‚ç‚¹

```python
from workflow_engine.core import Node, ParameterSchema, WorkflowContext, register_node

@register_node('batch_processor')
class BatchProcessorNode(Node):
    """æ‰¹é‡æ•°æ®å¤„ç†èŠ‚ç‚¹"""
    
    INPUT_PARAMS = {
        "data_list": ParameterSchema(
            is_streaming=False,
            schema="list"
        )
    }
    
    OUTPUT_PARAMS = {
        "processed_list": ParameterSchema(
            is_streaming=False,
            schema="list"
        ),
        "statistics": ParameterSchema(
            is_streaming=False,
            schema={
                "total": "integer",
                "processed": "integer",
                "failed": "integer"
            }
        )
    }
    
    async def execute_async(self, context: WorkflowContext):
        """æ‰¹é‡å¤„ç†æ•°æ®"""
        # è·å–è¾“å…¥
        data_list = self.get_input_value("data_list")
        
        context.log(f"å¼€å§‹å¤„ç† {len(data_list)} æ¡æ•°æ®")
        
        # å¤„ç†æ•°æ®
        processed_list = []
        failed_count = 0
        
        for item in data_list:
            try:
                processed_item = self._process_item(item)
                processed_list.append(processed_item)
            except Exception as e:
                context.log(f"å¤„ç†å¤±è´¥: {e}", level="WARNING")
                failed_count += 1
        
        # è®¾ç½®è¾“å‡º
        self.set_output_value("processed_list", processed_list)
        self.set_output_value("statistics", {
            "total": len(data_list),
            "processed": len(processed_list),
            "failed": failed_count
        })
        
        context.log(f"å¤„ç†å®Œæˆ: æˆåŠŸ {len(processed_list)}, å¤±è´¥ {failed_count}")
    
    def _process_item(self, item):
        """å¤„ç†å•ä¸ªæ•°æ®é¡¹"""
        # å®é™…å¤„ç†é€»è¾‘
        return item
```

### ç¤ºä¾‹ 3ï¼šæ··åˆæµå¼/éæµå¼èŠ‚ç‚¹

```python
from workflow_engine.core import Node, ParameterSchema, StreamChunk, WorkflowContext, register_node
import asyncio

@register_node('hybrid_node')
class HybridNode(Node):
    """æ··åˆèŠ‚ç‚¹ï¼šæµå¼è¾“å…¥ + éæµå¼é…ç½®"""
    
    INPUT_PARAMS = {
        "stream_data": ParameterSchema(
            is_streaming=True,
            schema={"value": "float"}
        ),
        "config_data": ParameterSchema(
            is_streaming=False,
            schema={"threshold": "float", "mode": "string"}
        )
    }
    
    OUTPUT_PARAMS = {
        "filtered_stream": ParameterSchema(
            is_streaming=True,
            schema={"value": "float", "passed": "boolean"}
        )
    }
    
    async def execute_async(self, context: WorkflowContext):
        """åˆå§‹åŒ–"""
        # è·å–éæµå¼é…ç½®
        config = self.get_input_value("config_data")
        self.threshold = config["threshold"]
        self.mode = config["mode"]
        
        context.log(f"æ··åˆèŠ‚ç‚¹å¯åŠ¨: threshold={self.threshold}, mode={self.mode}")
        
        try:
            await asyncio.sleep(float('inf'))
        except asyncio.CancelledError:
            context.log("æ··åˆèŠ‚ç‚¹å…³é—­")
    
    async def on_chunk_received(self, param_name: str, chunk: StreamChunk):
        """å¤„ç†æµå¼è¾“å…¥"""
        if param_name == "stream_data":
            value = chunk.data["value"]
            
            # æ ¹æ®é…ç½®è¿‡æ»¤æ•°æ®
            passed = value >= self.threshold
            
            if passed or self.mode == "all":
                await self.emit_chunk("filtered_stream", {
                    "value": value,
                    "passed": passed
                })
```

---

## æ€»ç»“

- **å‚æ•°åœ¨ä»£ç ä¸­å®šä¹‰**ï¼šä½¿ç”¨ `INPUT_PARAMS` å’Œ `OUTPUT_PARAMS` ç±»å±æ€§
- **æ”¯æŒä¸‰ç§æ³¨å†Œæ–¹å¼**ï¼šæ‰‹åŠ¨ã€è‡ªåŠ¨ã€è£…é¥°å™¨
- **æµå¼å’Œéæµå¼æ··åˆ**ï¼šæ ¹æ®ä¸šåŠ¡éœ€æ±‚é€‰æ‹©åˆé€‚çš„å‚æ•°ç±»å‹
- **å®Œå–„çš„ç±»å‹éªŒè¯**ï¼šå¼•æ“è‡ªåŠ¨éªŒè¯å‚æ•°ç»“æ„åŒ¹é…
- **æ¸…æ™°çš„ç”Ÿå‘½å‘¨æœŸ**ï¼šç†è§£èŠ‚ç‚¹æ‰§è¡Œæµç¨‹å’Œèµ„æºç®¡ç†

å¼€å§‹å¼€å‘æ‚¨çš„è‡ªå®šä¹‰èŠ‚ç‚¹å§ï¼ğŸš€

